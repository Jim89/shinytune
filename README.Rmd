---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# shinytune

<!-- badges: start -->
<!-- badges: end -->

The goal of shinytune is to make it easy to explore `tune` objects, similar to
`shinystan`.

To do this I need to:

* ~~Figure out exactly what `tune` is producing~~
* Figure out how to deal with `tune_` outputs which contain the (optional)
`.predictions` and/or `.extracts` information
* Think about some sensible summaries/visualisations/explorations that could
be applied to that
* Compare to `shinystan` for reference (I don't want to anchor
too strongly to it, though)

## Exploring `tune`

Firstly, I need to figure out what  `tune` object actually contains.

Let's create one following the Getting Started guide on
the [tune
website](https://tidymodels.github.io/tune/articles/getting_started.html).

```{r}
library(tidymodels)
library(tune)
```

### Create the tune objects

Let's just create the whole set of outputs for the Getting Started vignette.

Firstly we'll set up the data:

```{r}
library(AmesHousing)

ames <- make_ames()

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(data_split)
ames_test  <- testing(data_split)
```

Then the baseline recipe

```{r}
ames_rec <- recipe(Sale_Price ~ Longitude + Latitude, data = ames_train) %>% 
  step_log(Sale_Price, base = 10) %>% 
  step_ns(Longitude, deg_free = tune("long_df")) %>% 
  step_ns(Latitude,  deg_free = tune("lat_df"))

ames_rec
```

Then update the parameters to use a better function with a wider range: 

```{r}
ames_param <- ames_rec %>% 
  parameters() %>% 
  update(
    long_df = spline_degree(), 
    lat_df = spline_degree()
  )

ames_param
```

Then we'll set up the (grid) search space of values for these parameters:

```{r spline-grid}
spline_grid <- grid_max_entropy(ames_param, size = 10)
spline_grid
```

Then our (linear) model:

```{r lm-mod}
lm_mod <- linear_reg() %>% 
    set_engine("lm")

lm_mod
```

Then we'll set up the cross validation scheme to search over:

```{r cv-splits}
set.seed(2453)
cv_splits <- vfold_cv(ames_train, v = 10, strata = "Sale_Price")
```

### `tune_grid()`

Then finally we'll do the tuning using `tune_grid()`:

```{r tune-grid, cache = TRUE}
ames_res <- tune_grid(
    ames_rec,
    model = lm_mod,
    resamples = cv_splits,
    grid = spline_grid
)
ames_res
```

Let's start out by looking at the class of those results:

```{r}
class(ames_res)
```

As detailed in the `?tune_grid` documentation, we have an updated resamples
result, but with some new info an an extra class. But ultimately it's still a
rectangle. Let's check the methods for the new class (the first one) to see what
might be available "for free".

```{r}
methods(class = class(ames_res)[[1]])
```

So the only built-in method for the `_results` is `autoplot()`. So there's not a
huge amount we get out using S3.

What does the `autoplot()` look like?

```{r tune-grid-autoplot}
autoplot(ames_res)
```

We've got the performance metrics over our parameter value(s).

Let's look at the extra columns in a bit more detail:

```{r}
ames_res$.metrics[[1]]
```

`.metrics`, contrains the error/summary metrics for each combination of the
parameters in our grid. Per the docs, the model type informs the default choice
of metric, but we can also specify what we're after with the `metrics` parameter
in `tune_grid()`.

The `.notes` column should contain extra warnings/errors that occurred during
execution.

```{r}
map_dfr(ames_res$.notes, I)
```

We didn't have any here.

The docs also show the `collect_metrics()` function for aggregating the error
metrics over the resamples. We can get summary (mean) values per grid
combination (the default):

```{r}
collect_metrics(ames_res)
```

Or not, in which case we get the exact metrics from each fold (maybe useful if
we want to compute our own summaries).

```{r}
collect_metrics(ames_res, FALSE)
```

The documentation then uses these result to plot/calcualte bespoke summaries to
inform modelling decisions, and the use of `tune` or other `tidymodels`
functions/packages seems to end.

In any case, the output seems reasonably simple: `tune_grid()` will return a set
of metrics (that may be customised via the `metrics`) argument for each
combination in the grid.

#### Adding predictions

Let's also consider using `control_grid()` to add the `.predictions` data to the
results of our tuning:

```{r tune-grid-pred, cache = TRUE}
ames_res_with_pred <- tune_grid(
    ames_rec,
    model = lm_mod,
    resamples = cv_splits,
    grid = spline_grid,
    control = control_grid(save_pred = TRUE)
)
ames_res_with_pred
```

Let's look at one value of predictions:

```{r}
ames_res_with_pred$.predictions[[1]]
```

So we have the prediction across many rows of the data, based on our resamples?

```{r}
count(ames_res_with_pred$.predictions[[1]], long_df, lat_df)
```

Yes, for each evaluation/parameter set, we've got  the out-of-sample predictions
(like the docs say, but it's nice to verify).

Let's use the built in helper to get them, too:

```{r}
collect_predictions(ames_res_with_pred)
```

As expected, over our grid of 10 lat/long DF combinations, we get, for each
resample, the out-of-sample predictions (so 10 values per observation in the
validation set across each resample fold).

Per the documentation, the column names may differ if we have a classification
model (e.g. `.pred_class`), so we'll need to be careful there if we're thinking
about a general framework.

#### Adding extractions

Let's instead use `control_grid()`'s `extract` argument. This is an "optional
function with at least one argument that can be used to retain arbitrary objects
from the model fit object, receipe, or other elements of the workflow".

Continuing, the help file for `tune_bayesian()` notes that:

> The control function contains an option (extract) that can be used to retain
any model or recipe that was created within the resamples. This argument should
be a function with a single argument. The value of the argument that is given to
the function in each resample is a workflow object (see `workflows::workflow()`
for more information). There are two helper functions that can be used to easily
pull out the recipe (if any) and/or the model: `extract_recipe()` and
`extract_model()`.

The fact that this can be arbitrary may mean this is hard to include in anything
like `shinytune` which should be _general_ and not too arbitrary, but let's
explore nonetheless. First off we'll extract the model:

```{r ames-res-mod, cache = TRUE}
ames_res_mod <- tune_grid(
    ames_rec,
    model = lm_mod,
    resamples = cv_splits,
    grid = spline_grid,
    control = control_grid(extract = extract_model)
)
ames_res_mod
```

Let's look at the first extract:

```{r}
ames_res_mod$.extracts[[1]]
```

So for each set of parameters in the grid, we ahve the model that was fit. Let's
look at the first one of _those_

```{r}
mod <- ames_res_mod$.extracts[[1]]$.extracts[[1]]
summary(mod)
```

So we get back the actual model that was fit. This could be really powerful,
allowing the user to explore individual models fit during the grid search. But
that might be hard to generalise into `shinytune`, and might be better saved for
bespoke work on the users part.

Let's do the same thing and extract the recipe instead:

```{r ames-res-rec, cache = TRUE}
ames_res_rec <- tune_grid(
    ames_rec,
    model = lm_mod,
    resamples = cv_splits,
    grid = spline_grid,
    control = control_grid(extract = extract_recipe)
)
ames_res_rec
```

Then let's again look at the `.extracts`:

```{r}
res <- ames_res_rec$.extracts[[1]]$.extracts[[1]]
res
```

So we get a _trained_ recipe back.

This may be useful to some users, but again might be a little hard to generalise
in something like `shinytune`, and I'm not sure exactly what the use would be in
a general-purpose exploration tool.

Finally, as we know the `extract` function is given a `workflow`, let's just
return that.

```{r ames-res-wf}
ames_res_wf <- tune_grid(
    ames_rec,
    model = lm_mod,
    resamples = cv_splits,
    grid = spline_grid,
    control = control_grid(extract = function(x) I(x))    
)
ames_res_wf
```

Then look at the extracted info:

```{r}
wf <- ames_res_wf$.extracts[[1]]$.extracts[[1]]
wf
```

What's in this workflow object (in addition to the trained recipe and the model itself, which we've already seen how to extract)?

```{r}
names(wf)
```

A few things, let's look at each.

```{r}
wf$pre
```

I'm not really sure what this is for the moment, but it looks like there's some
pre-training information in there about the recipe, (some of) the transformed
data, and the outcome variable.

```{r}
wf$fit
```

The `fit` object looks like it holds the `parsnip` model object that
`extract_model()` would pull out.

```{r}
wf$post
```

Post looks like it probably gives the post-fitting items from the workflow
(there are none here). [I 2x checked the `workflow` documentation, and it will,
it's just that none of the post-processing steps are currently implemented.]

Finally, we have trained, a simple logical indicating (I assume) if the workflow
has been trained:

```{r}
wf$trained
```

Overall then, there's a fair bit that _could_ be done with the workflow object,
so it might be hard to generalise that in `shinytune`.

### `tune_bayes()`

An alternative approach in the getting started document is to use Bayesian
Optimisation to select the parameters.

The actual model used in the docs (kNN) is more complex than the linear
regression above, so let's stick with the simple case for now.

First we set up the workflow:

```{r}
library(workflows)
lm_wflow <- workflow() %>% 
    add_model(lm_mod) %>% 
    add_recipe(ames_rec)

lm_wflow
```

Then add the parameters:

```{r}
lm_param <- parameters(lm_wflow)
lm_param
```

Then we'll set up the Bayesian optimisation:

```{r tune-bayes, cache = TRUE}
ctrl <- control_bayes(verbose = TRUE)
set.seed(8151)
lm_search <- tune_bayes(
    lm_wflow, 
    resamples = cv_splits, 
    initial = 5, # Initial results to compare against
    iter = 20, # Max searchable iterations
    param_info = lm_param, 
    control = ctrl
)
```

Let's have a look at the results:

```{r}
lm_search
```

This looks similar to the output from grid search, but there's a lot more of it,
because we have results by fold by iteration (`.iter`).

Is the class the same?

```{r}
class(lm_search) == class(ames_res)
```

Yes, identical.

Let's try that same autoplot, then:

```{r tune-bayes-autoplot}
autoplot(lm_search, "performance")
```

We get something very similar, but with more data to show (as we have multiple
iterations).

And what about a single metrics object:

```{r}
lm_search$.metrics[[1]]
```

This is similar again, for each fold/resample in each iteration we get a set of
metrics over a range of values of the parameters.

And what about `collect_metrics()`?

```{r}
collect_metrics(lm_search)
```

Again, something very similar, although now we get additional values, again
because of iterations (we get the average of the metric for each combination of
parameters in each iteration):

```{r}
lm_search %>% 
    collect_metrics() %>% 
    arrange(.iter, long_df, lat_df) %>% 
    filter(.metric == "rmse")
```

So, given the two main ways to `tune` a model, we can see we have very similar
results. Next steps are to figure out the case where the `tune_` output contains
`.predictions` and/or `.extracts` (which will be present if the relevant
arguments are set in the `control_` functions used to control the search).

### `autoplot()` methods

Let's also look at the `autoplot()` methods for the `tune_` objects.

The best place to start is the documentation:

```{r}
?tune::autoplot.tune_results
```

We can see that their is a `type` argument, giving 3 different plots.
`"marginals"` shows predictors vs. performance (useful). The latter two options
are only available for `tune_bayes()` and are: `"parameters"` for the parameters
vs. the iteration, and `"performance"` to show performance at each iteration.

We can also set `metric`, chosing the metric to plot (presumably from the
`metrics` argument in `tune_`) and the `width` to show the width of the
confidence bands when `type` is `"performance"`.

Let's see those three plots.

Let's look at the marginals plot from the original grid search:

```{r}
autoplot(ames_res)
```

And the Bayesian search:

```{r}
autoplot(lm_search)
```

Two very similar plots, we just have the extra results (for iterations) in the
Bayesian output.

Let's look at the other two plots for the Bayesian object

```{r}
autoplot(lm_search, "parameters")
```

The `"parameters"` plot could be useful to show how well the process has/hasn't
explored the potential parameter space.

```{r}
autoplot(lm_search, "perf")
```

The performance plot is useful, too. The width is just controlling the visual
width of the error bars (i.e. the horizontal bit), rather than the width of the
interval.

```{r}
autoplot(lm_search, "perf", width = .25)
```

So these plots look to be useful-ish, but some custom ones may be more useful.

It's also worth noting that per the documentation:

> A single categorical tuning parameter is supported when other numeric
parameters are also in the results. Any number of numeric tuning parameters can
be used.

Consulting the actual source code shows that for the marginal plot:

* More than 2 non-numeric parameters are not supported (`"Currently cannot
autoplot grids with 2+ non-numeric parameters."`), probably due to
dimensionality problems?
* _Only_ non-numeric parameters is not supported: `"Currently cannot autoplot
grids with only non-numeric parameters."`, possibly due to problems creating the
plot.

No similar restrictions look to be in place for the parameters and/or
performance plots.

### Additional functions

There are a few other built-in functions in `tune` that might be useful/serve as
inspiration for `shinytune`.

* `last_fit()` and `fit_resamples()` might be useful in some cases (e.g. for
interactive re-tuning?)
* `show_best()` and `select_best()` might be useful to grab the best parameter
combinations
* the `select_by*()` functions can analyse the output of `tune_()` functions
* `finalize_()` functions for updating the model/workflow/recipe from the
`tune_()` output

#### `select_` and `show`

Let's look at the `show_` and `select_` functions first. We'll look at using
them on the output of `tune_grid()` and `tune_bayes()`

We just need to select the (error) metric to measure by:

```{r}
show_best(ames_res, "rmse")
```

```{r}
show_best(lm_search, "rmse")
```

`show_best()` shows the top `n` best models, whilst `select_best()` extracts the
best record for a given metric (whether we want that metric minimised or
maximised).

```{r}
select_best(ames_res, "rmse", maximize = FALSE)
```

```{r}
select_best(lm_search, "rmse", FALSE)
```

In both cases we just get back the parameter values.

Let's also explore the `select_by_` functions.

Here, we need to provide additional sorting criteria to penalise more complex
models, as this seeks to provide the simplest model that's within a certain
percentage-loss from the best one.

```{r}
select_by_pct_loss(ames_res, long_df, lat_df, metric = "rmse", maximize = FALSE)
```

The `limit` parameter let's us control the percentage point difference in loss
we're willing to tolerate.

```{r}
select_by_one_std_err(ames_res, long_df, lat_df, metric = "rmse", maximize = FALSE)
```

All 4 of these functions feel like they would be useful to enable interactive
exploration of the results in `shinytune`.

#### `finalize_` functions

Let's also look at the `finalize_` functions, which need a recipe, model, or
workflow, and an output from `select_best()` (or similar) that contain the
parameters in the model/workflow/recipe that should be updated/finalised.

E.g.

```{r}
finalize_recipe(ames_rec, select_best(ames_res, "rmse", FALSE))
```

```{r}
finalize_workflow(lm_wflow, select_best(lm_search, "rmse", F))
```

These may be less useful for `shinytune` in the first-instance, as they would
require `shinytune` to also accept the model/recipe/workflow that `tune_`
operated on.

#### `last_fit()` and `fit_resamples()`

`last_fit()` is only useful once we have finalised a model. 

E.g.

```{r}
final_wf <- finalize_workflow(lm_wflow, select_best(lm_search, "rmse", F))
last_fit(final_wf, split = data_split)
```

Different `.metrics` can be calculated (again using a
`yardstick::metric_set()`), as needed.

`fit_resamples()` doesn't do any tuning, but will (re)fit a model across many
resamples, so could be useful to let the user tweak parameters in an interface
and get the results back? (Probaby overly complicated actually).

But this could be useful for getting an estimate of variability in performance
over resamples from the final model?

```{r}
final_wf %>% 
  fit_resamples(cv_splits) %>% 
  collect_metrics(summarize = FALSE) %>% 
  filter(.metric == "rmse") %>% 
  qplot(.estimate, data = .)
```

Ish/yes/no. Probably these should be considered out of scope for now.
